{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# ASSIGNMENT 3                                            McGill:COMP766-2  Winter 2020 \n",
    "Student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The objective of this assignment is to implement loopy belief propagation, and mean-field method for the Ising model, which we saw last time. Below is a review from the last assignment.\n",
    "\n",
    "#### Ising model\n",
    "The Ising model is fully expressed using a symmetric adjacency matrix $A$ (```adj```):\n",
    "\\begin{align}\n",
    "  p(x) = \\frac{1}{Z} \\exp(\\sum_{i} A_{i,i}x_i + \\sum_{i,j < i} x_i A_{i,j}x_j) \\quad x_i \\in \\{-1,+1\\}\n",
    "\\end{align}\n",
    "In words, the diagonal of $A$ represents the local field and interactions are captured by the remaining non-zero elements of $A$ (It is customary to have a negative sign in front of the summation, but for simplicity we dropped it.)\n",
    "The graph structure can also be inferred from the sparsity patter of $A$.\n",
    "Here, we are interested in deterministic sum-product inference.\n",
    "\n",
    "\n",
    "## Problem 1.  Loopy Belief Propagation\n",
    "\n",
    "### 1.a. (10 pts) \n",
    "Complete the implementation of loopy-BP below. (The base ```Inference``` class as well as ```CliqueTreeInference``` are included in the ```inference.py``` helper file.) \n",
    "Recall that the BP message $m_{i \\to j}$ for a pairwise MRF is the product of all incoming messages, and the relevant factors, marginalized over $x_i$. For the Ising model this is:\n",
    "\\begin{align} \n",
    "m_{i \\to j}(x_j) \\propto (1-\\alpha) m^{old}_{i \\to j}(x_j) \\, + \\, \\alpha \\sum_{x_i \\in \\{-1,+1\\}} \\left ( \\exp(x_i A_{i,i} + x_i A_{i,j} x_j) \\prod_{k \\in Nb(i)-j} m_{k \\to i}(x_i) \\right )\n",
    "\\end{align}\n",
    "where $\\alpha$ called the damping factor helps with the convergence. After convergence the marginals are:\n",
    "\\begin{align}\n",
    "p(x_i) \\propto \\exp(x_i A_{i,i}) \\prod_{j \\in Nb(i)} m_{j \\to i}(x_i)\n",
    "\\end{align}\n",
    "\n",
    "In the implementation below:\n",
    "Try to use only a single loop, which runs for a maximum of ```self.max_iters```. If you find this too challenging you can also loop over individual messages.\n",
    "The iterations should terminate as soon as the maximum change in the absolute value of the marginals in two iterations goes below ```self.eps```. To help with convergence you can damp (smoothed) your message update using the damping parameter ```self.damping``` \n",
    "\n",
    "#### using log-ratio of messages\n",
    "When dealing with a binary domain, it is more efficient to only track a single probability value (rather two values for the message $m_{i \\to j}(-1), m_{i \\to j}(+1)$ in the case of Ising model. Because this is redundant.) In fact, for numerical stability, it is convenient to track only the log-ratio $\\mu_{i \\to j} = \\log \\left( \\frac{m_{i \\to j}(+1)}{m_{i \\to j}(-1)} \\right )$. One could then try and rewrite the message update using $\\mu$ rather original $m$. This enables us to only track $\\mu$, and after convergence produce marginals using these log-ratios. we can also write the marginals for this setting. For this you need to substitute $m(1) = \\frac{1}{1 + e^{-\\mu}} = \\sigma(\\mu)$ and $m(-1) = 1 - \\sigma(\\mu)$ in the update equation above and simplify (if you decide __not__ to use log-ratios you may need to update other parts of the code below, which was written based on this assumption)\n",
    "\n",
    "### 1.a. solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import networkx as nx\n",
    "np.random.seed(12345)\n",
    "import timeit\n",
    "#removing some warnings!\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "from utils import tensor_mult, draw_graph, logistic\n",
    "from inference import Inference, CliqueTreeInference\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "class LBPInference(Inference):\n",
    "    \"\"\"\n",
    "    implements sum-product loopy belief propagation inference \n",
    "    for  p(x) \\propto exp(+\\sum_{i,j<i} x_i x_j adj[i,j] + \\sum_{i} x_i adj[i,i]) for  x_i \\in {-1,+1}\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 adj,\n",
    "                 verbosity=0,\n",
    "                 max_iters=1000,  # max number of iterations\n",
    "                 damping=.1,  # damping=0 means no damping\n",
    "                 eps=1e-16,  # threshold for convergence\n",
    "    ):\n",
    "        Inference.__init__(self, adj, verbosity=verbosity)\n",
    "        self.damping = damping\n",
    "        self.max_iters = max_iters\n",
    "        # (i,j)th element is the log-ratio of message i->j\n",
    "        self.msg_new = None\n",
    "        self.msg_old = None\n",
    "        self.marginals = None\n",
    "        self.converged = False\n",
    "        self.mask = None  # zero-one mask for the adjacency (zero diagonals)\n",
    "        self.eps = eps\n",
    "        self._initialize()\n",
    "        self.adj = adj\n",
    "\n",
    "    def update_adjacency(self, new_adj):\n",
    "        self.adj = new_adj\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        n = self.adj.shape[0]\n",
    "        self.msg_new = np.zeros((n , n))\n",
    "        self.msg_old = np.zeros((n , n))\n",
    "        self.marginals = np.zeros(n)\n",
    "        mask = (self.adj != 0)\n",
    "        np.fill_diagonal(mask, 0)\n",
    "        self.mask = mask\n",
    "        self.converged = False\n",
    "\n",
    "    def _apply_damping(self, msg_new, msg_old):\n",
    "        #convert to probability\n",
    "        msg_new_p = logistic(msg_new)\n",
    "        msg_old_p = logistic(msg_old)\n",
    "        msg_p = msg_old_p * (self.damping) + msg_new_p * (1. - self.damping)\n",
    "        # convert back to log-ratio\n",
    "        msg = np.log(msg_p) - np.log(1. - msg_p)\n",
    "        return msg\n",
    "    \n",
    "    def _inference(self):\n",
    "        if self._verbosity > 0:\n",
    "            print(\"running BP\", flush = True)\n",
    "        \n",
    "        # >>>> YOUR CODE HERE >>>>>>>\n",
    "\n",
    "        # >>>> YOUR CODE HERE >>>>>>>\n",
    "        \n",
    "        self.marginals = new_marginals\n",
    "        if self._verbosity > 0:\n",
    "            print(\"finished after {} iterations!\".format(t), flush = True)\n",
    "                \n",
    "    def get_marginal(self, target):\n",
    "        if not self.converged:\n",
    "            self._inference()\n",
    "            self.converged = True\n",
    "        mrg = logistic(self.marginals)[target]\n",
    "        return mrg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "If the implementation above is correct, it should return exact marginals for the tree structure below. You can use this to debug your code (We are comparing the ```LBPInference``` marginals against the exact values returned by ```CliqueTreeInference```.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#build a tree \n",
    "tree = nx.balanced_tree(3, 4)\n",
    "n = nx.number_of_nodes(tree)\n",
    "mask = np.array(nx.adjacency_matrix(tree).todense())\n",
    "# create a random interaction over the edges\n",
    "np.fill_diagonal(mask, 1.)\n",
    "adj = mask * np.random.randn(n , n)\n",
    "# make it symmetric\n",
    "zero_inds = np.tril_indices_from(adj,-1)\n",
    "adj[zero_inds] = 0        \n",
    "adj += adj.T.copy()\n",
    "# do inference\n",
    "true_margs = CliqueTreeInference(adj).get_marginal(list(range(adj.shape[0])))            \n",
    "bp_margs = LBPInference(adj).get_marginal(list(range(adj.shape[0])))\n",
    "assert np.mean(np.abs(true_margs - bp_margs)) < 1e-10, \"LBP marginals are not exact\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 1.c. (5 pts) \n",
    "apply it to $nÃ—n$ 2D-grids (```grid_2d_graph```), of increasing size $n \\in \\{2, 4, 6, 8, 15\\}$,\n",
    "and compare its run-time with clique-tree. The x-axis in the plot is the number of nodes $n$ and y-axis is the run-time (use logarithmic scale). Repeat your experiment at least 10 times to produce an error-bars for your plot.\n",
    "\n",
    "### 1.c. solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# >>>> YOUR CODE HERE >>>>>>>\n",
    "\n",
    "# >>>> YOUR CODE HERE >>>>>>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Problem 2. Mean Field (15 pts)\n",
    "\n",
    "### 2.a. (8 pts)\n",
    "Complete the implementation of naive mean-field below. \n",
    "You can refer to slides for derivation of the updates for the Ising model.\n",
    "In the implementation below: \n",
    "Try to use only a single loop, which repeats for a a maximum of ```self.max_iters```. \n",
    "The loop can terminate as soon as the maximum of the absolute value of change in the marginals \n",
    "between two iterations goes below ```self.eps```.\n",
    "For better convergence try damping (smoothing) the updates, using the damping parameter ```self.damping```.\n",
    "\n",
    "### 2.a. solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFInference(Inference):\n",
    "    \"\"\"\n",
    "    implements parallel update of naive mean-field for the Ising model\n",
    "    mean-field is guaranteed to converge to a local optima, when updating \n",
    "    one coordinate at a time. Here, since for efficiency/simplicity we update all coordinates simultaneously,\n",
    "    we may need to use damping to help with convergence\n",
    "    for  p(x) \\propto exp(+\\sum_{i,j<i} x_i x_j adj[i,j] + \\sum_{i} x_i adj[i,i]) for  x_i \\in {-1,+1}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 adj,  # adjacency matrix for the Ising model\n",
    "                 verbosity=0,\n",
    "                 max_iters=1000,  # max number of iterations\n",
    "                 eps=1e-16,  # threshold for convergence\n",
    "                 damping=.5, # because we are using parallel updates, 0 means no damping\n",
    "    ):\n",
    "        Inference.__init__(self, adj, verbosity=verbosity)\n",
    "        self.damping = damping\n",
    "        self.max_iters = max_iters\n",
    "        # (i,j)th element is the log-ratio of message i->j\n",
    "        self.means = None\n",
    "        self.converged = False\n",
    "        self.adj_masked = None  # adjacency matrix with zero diagonals\n",
    "        self.eps = eps\n",
    "        self._initialize()\n",
    "\n",
    "    def update_adjacency(self, new_adj):\n",
    "        self.adj = new_adj\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        n = self.adj.shape[0]\n",
    "        self.means = np.zeros(n)\n",
    "        adj_masked = self.adj.copy()\n",
    "        np.fill_diagonal(adj_masked, 0)\n",
    "        self.adj_masked = adj_masked\n",
    "        self.converged = False\n",
    "\n",
    "    def _inference(self):\n",
    "        if self._verbosity > 0:\n",
    "            print(\"running MF\", flush = True)\n",
    "        # >>>> YOUR CODE HERE >>>>>>>\n",
    "\n",
    "        # >>>> YOUR CODE HERE >>>>>>>\n",
    "        if self._verbosity > 0:\n",
    "            print(\"finished after {} iterations!\".format(t), flush = True)\n",
    "                \n",
    "    def get_marginal(self, target):\n",
    "        if not self.converged:\n",
    "            self._inference()\n",
    "            self.converged = True\n",
    "        # converting the mean ([-1,+1] range) to the p(x_i = 1) [0,1] range\n",
    "        mrg = (self.means[target] + 1.)/2.\n",
    "        return mrg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b (7 pts)\n",
    "Now, we can can compare the accuracy of belief propagation and mean-field in approximate inference.\n",
    "For this experiment, we use a fixed $15\\times15$ 2D grid (without periodic boundaries).\n",
    "To control the difficulty of the inference problem, we adjust the temperature parameter $T$.\n",
    "This corresponds to multiplication of the adjacency matrix $A$ with a constant. Large temperatures,\n",
    "correspond to lower values of interaction $A_{ij}$, which reduces interdependency of variables:\n",
    "\\begin{align}\n",
    "  p^{\\frac{1}{T}}(x) = \\frac{1}{Z(T)} \\exp(\\frac{1}{T}\\sum_{i} A_{i,i}x_i + \\frac{1}{T} \\sum_{i,j < i} x_iA_{i,j}x_j) \\quad x_i \\in \\{-1,+1\\}\n",
    "\\end{align}\n",
    "We sample the non-zero elements of $A$, from a standard normal distribution, and adjust\n",
    "the difficulty of approximate inference by changing $T \\in \\{2^{-4},2^{-3},\\ldots, 2^{9}\\}$.\n",
    "Use the mean-absolute-error of all the univariate marginals as a measure of accuracy of approximate inference, and plot temperature-vs-accuracy for both naive mean-field and loopy belief propagation.\n",
    "Note that to measure the _error_, first you have to perform _exact inference_ on the same graphical model using ```CliqueTreeInference```, to obtain the exact marginals.\n",
    "Repeat the experiment 10 times and produce error-bars for your difficulty-vs-accuracy plot (use logarithmic scale for both x and y).\n",
    "Summarize what you see in the plot.\n",
    "\n",
    "### 2.b solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
